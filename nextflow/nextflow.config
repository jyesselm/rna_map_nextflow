/*
 * Nextflow configuration file for RNA MAP pipeline
 * 
 * This configures Nextflow to use SLURM, manage resources, and clean up files.
 * 
 * To customize tool arguments (FastQC, Trim Galore, Bowtie2), create a params.config file
 * or use the provided params.config template:
 *   nextflow run main.nf -c params.config --fasta ref.fasta --fastq1 reads.fastq
 */

// Default parameters (can be overridden in main.nf or params.config)
params.max_cpus = 16
params.max_memory = "32 GB"
params.max_time = "24h"

// SLURM executor configuration
process {
    executor = 'slurm'
    
    // Default resource limits
    cpus = { task.cpus ?: 4 }
    memory = { task.memory ?: '8 GB' }
    time = { task.time ?: '2h' }
    
    // SLURM-specific settings
    clusterOptions = { 
        def opts = []
        if (params.account) {
            opts << "--account=${params.account}"
        }
        opts << "--partition=${params.partition}"
        opts << "--job-name=rna_map_${task.name}"
        opts << "--output=logs/slurm-%j.out"
        opts << "--error=logs/slurm-%j.err"
        return opts.join(' ')
    }
    
    // Process labels for different resource requirements
    withLabel: 'process_low' {
        cpus = 2
        memory = '4 GB'
        time = '1h'
    }
    
    withLabel: 'process_medium' {
        cpus = 4
        memory = '8 GB'
        time = '2h'
    }
    
    withLabel: 'process_high' {
        cpus = { params.max_cpus }
        memory = { params.max_memory }
        time = { params.max_time }
    }
    
    // Cleanup intermediate files after successful completion
    cleanup = true
    
    // Error handling
    errorStrategy = 'retry'
    maxRetries = 3
    
    // Module loading (customize for your cluster)
    beforeScript = '''
        module load bowtie2/2.3 || true
        module load trim_galore || true
        module load fastqc || true
        module load anaconda || true
        conda activate rna_map || true
    '''
}

// Work directory (use scratch space if available)
workDir = System.getenv('SCRATCH') ? "${System.getenv('SCRATCH')}/nextflow-work" : "${System.getenv('HOME')}/nextflow-work"

// Results directory (will be set by params.output_dir in workflow)

// Cleanup old work directories
cleanup = true

// Logging
log {
    enabled = true
    file = "logs/nextflow.log"
    level = 'info'
}

// Report generation
report {
    enabled = true
    file = "reports/execution_report.html"
    overwrite = true
}

// Timeline report
timeline {
    enabled = true
    file = "reports/timeline.html"
    overwrite = true
}

// Trace file
trace {
    enabled = true
    file = "reports/trace.txt"
    overwrite = true
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes,vol_ctxt,inv_ctxt'
}

// DAG visualization
dag {
    enabled = true
    file = "reports/pipeline_dag.html"
    overwrite = true
}

// Manifest
manifest {
    name = 'RNA MAP'
    version = '0.4.1'
    description = 'RNA mutational profiling (MaP) analysis pipeline'
    author = 'Yesselman Lab'
    homePage = 'https://github.com/YesselmanLab/rna_map'
}

// Profiles for different environments
profiles {
    // Local execution (for testing)
    local {
        process.executor = 'local'
        process.cpus = { Math.min(task.cpus ?: 4, Runtime.runtime.availableProcessors()) }
        process.beforeScript = '''
            if [ -n "$CONDA_PREFIX" ]; then
                export PATH="$CONDA_PREFIX/bin:$PATH"
            elif command -v conda &> /dev/null; then
                eval "$(conda shell.bash hook 2>/dev/null || conda shell.zsh hook 2>/dev/null || true)"
                conda activate rna_map_nextflow 2>/dev/null || true
            elif command -v mamba &> /dev/null; then
                eval "$(mamba shell.bash hook 2>/dev/null || mamba shell.zsh hook 2>/dev/null || true)"
                mamba activate rna_map_nextflow 2>/dev/null || true
            fi
        '''
        process.memory = '8 GB'
        // Override max_cpus for local mode
        params.max_cpus = 4
        // Don't check versions in local mode (faster for testing)
        process.beforeScript = '''
            # Local mode - assume tools are in PATH
        '''
    }
    
    // HPC with SLURM (default)
    slurm {
        process.executor = 'slurm'
        process.clusterOptions = { 
            def opts = []
            if (params.account) {
                opts << "--account=${params.account}"
            }
            opts << "--partition=${params.partition}"
            return opts.join(' ')
        }
    }
    
    // Docker execution
    docker {
        docker.enabled = true
        docker.runOptions = '-u $(id -u):$(id -g)'
    }
    
    // Singularity execution
    singularity {
        singularity.enabled = true
        singularity.autoMounts = true
    }
}

