/*
 * Nextflow configuration file for RNA MAP pipeline
 * 
 * This is the main configuration file. It includes base parameters and sets up profiles.
 * 
 * Usage:
 *   nextflow run main.nf -profile local --fasta ref.fasta --fastq1 reads.fastq
 *   nextflow run main.nf -profile slurm --fasta ref.fasta --fastq1 reads.fastq
 * 
 * To customize tool arguments, use conf/base.config or create your own config file.
 */

// Include base configuration (parameters)
includeConfig 'conf/base.config'

// Default parameters (can be overridden in profiles or command line)
params.max_cpus = 16
params.max_memory = "32 GB"
params.max_time = "24h"

// SLURM executor configuration
process {
    executor = 'slurm'
    
    // Default resource limits
    cpus = { task.cpus ?: 4 }
    memory = { task.memory ?: '8 GB' }
    time = { task.time ?: '2h' }
    
    // SLURM-specific settings
    clusterOptions = { 
        def opts = []
        opts << "--job-name=rna_map_${task.name}"
        opts << "--output=logs/slurm-%j.out"
        opts << "--error=logs/slurm-%j.err"
        return opts.join(' ')
    }
    
    // Process labels for different resource requirements
    withLabel: 'process_low' {
        cpus = 2
        memory = '4 GB'
        time = '1h'
    }
    
    withLabel: 'process_medium' {
        cpus = 4
        memory = '8 GB'
        time = '2h'
    }
    
    withLabel: 'process_high' {
        cpus = { params.max_cpus }
        memory = { params.max_memory }
        time = { params.max_time }
    }
    
    // Cleanup intermediate files after successful completion
    cleanup = true
    
    // Error handling
    errorStrategy = 'retry'
    maxRetries = 3
    
    // Module loading (customize for your cluster)
    beforeScript = '''
        module load bowtie2/2.3 || true
        module load trim_galore || true
        module load fastqc || true
        module load anaconda || true
        conda activate rna_map || true
    '''
}

// Work directory (use scratch space if available)
workDir = System.getenv('SCRATCH') ? "${System.getenv('SCRATCH')}/nextflow-work" : "${System.getenv('HOME')}/nextflow-work"

// Results directory (will be set by params.output_dir in workflow)

// Cleanup old work directories
cleanup = true

// Logging
log {
    enabled = true
    file = "logs/nextflow.log"
    level = 'info'
}

// Report generation
report {
    enabled = true
    file = "reports/execution_report.html"
    overwrite = true
}

// Timeline report
timeline {
    enabled = true
    file = "reports/timeline.html"
    overwrite = true
}

// Trace file
trace {
    enabled = true
    file = "reports/trace.txt"
    overwrite = true
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes,vol_ctxt,inv_ctxt'
}

// DAG visualization
dag {
    enabled = true
    file = "reports/pipeline_dag.html"
    overwrite = true
}

// Manifest
manifest {
    name = 'RNA MAP'
    version = '0.4.1'
    description = 'RNA mutational profiling (MaP) analysis pipeline'
    author = 'Yesselman Lab'
    homePage = 'https://github.com/YesselmanLab/rna_map'
}

// Profiles for different environments
profiles {
    // Local execution (for testing) - uses conf/local.config
    local {
        includeConfig 'conf/local.config'
    }
    
    // HPC with SLURM (default) - uses conf/slurm.config
    slurm {
        includeConfig 'conf/slurm.config'
    }
    
    // SLURM + Local hybrid - submit to SLURM but use local executor for fast tasks
    slurm_local {
        includeConfig 'conf/slurm_local.config'
    }
    
    // SLURM optimized - for slow/long-running tasks with queue batching
    slurm_optimized {
        includeConfig 'conf/slurm_optimized.config'
    }
    
    // SLURM hybrid - local for fast tasks, SLURM for slow tasks
    slurm_hybrid {
        includeConfig 'conf/slurm_hybrid.config'
    }
    
    // Docker execution
    docker {
        docker.enabled = true
        docker.runOptions = '-u $(id -u):$(id -g)'
        includeConfig 'conf/base.config'
    }
    
    // Singularity execution
    singularity {
        singularity.enabled = true
        singularity.autoMounts = true
        includeConfig 'conf/base.config'
    }
}

